{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWKIoGdNlQde"
      },
      "outputs": [],
      "source": [
        "!pip install mlxtend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCT5dWT0lWtm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import skimage.io\n",
        "import copy\n",
        "import random\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from skimage import filters\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import pickle\n",
        "from scipy.stats import kendalltau\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from sklearn.utils import resample\n",
        "from scipy.stats import norm, gaussian_kde\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.datasets import load_wine\n",
        "import itertools\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import VarianceThreshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiF9610fmT9G"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "\n",
        "def remove_highly_correlated_features_pd(X, threshold):\n",
        "    # Convert X to a pandas DataFrame\n",
        "    # Calculate the correlation matrix\n",
        "    df = pd.DataFrame(X)\n",
        "    corr_matrix = df.corr().abs()\n",
        "    # Create a mask to remove the upper triangle of the correlation matrix\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "    # Set the upper triangle values to NaN\n",
        "    corr_matrix.mask(mask, inplace=True)\n",
        "\n",
        "    # Find the highly correlated features\n",
        "    cols_to_drop = [column for column in corr_matrix.columns if any(corr_matrix[column] > threshold)]\n",
        "\n",
        "    # Drop the highly correlated features from the DataFrame\n",
        "    reduced_df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "    # Convert the reduced DataFrame back to numpy array\n",
        "    X_reduced = reduced_df.to_numpy()\n",
        "\n",
        "    return X_reduced, cols_to_drop\n",
        "\n",
        "\n",
        "def calculate_entropy(data):\n",
        "    if np.var(data) == 0:\n",
        "        return 0\n",
        "\n",
        "    scipy_kernel = gaussian_kde(data)\n",
        "\n",
        "    #  We calculate the bandwidth for later use\n",
        "    optimal_bandwidth = scipy_kernel.factor * np.std(data)\n",
        "\n",
        "    # Calculate KDE for the entire dataset\n",
        "    kde = gaussian_kde(data, bw_method=optimal_bandwidth)\n",
        "\n",
        "    # Create a range of values to represent the KDE\n",
        "    x = np.linspace(np.min(data), np.max(data), 1000)\n",
        "\n",
        "    # Evaluate the density at each point in the range\n",
        "    density = kde(x)\n",
        "\n",
        "    # Normalize the density function\n",
        "    normalized_density = density / np.sum(density * (x[1] - x[0]))\n",
        "\n",
        "    # Calculate the probabilities of positive and negative values\n",
        "    positive_probability = np.sum(normalized_density[x >= 0] * (x[1] - x[0]))\n",
        "    negative_probability = np.sum(normalized_density[x < 0] * (x[1] - x[0]))\n",
        "\n",
        "    if positive_probability == 0 or negative_probability == 0:\n",
        "        sign_entropy = 0\n",
        "    else:\n",
        "        sign_entropy = -positive_probability * np.log2(positive_probability) \\\n",
        "                       - negative_probability * np.log2(negative_probability)\n",
        "\n",
        "    return sign_entropy\n",
        "\n",
        "\n",
        "def calculate_entropies(result_matrix):\n",
        "    sign_entropies = []\n",
        "    for column in range(result_matrix.shape[1]):\n",
        "        data = result_matrix[:, column]\n",
        "        sign_entropy = calculate_entropy(data)\n",
        "        sign_entropies.append(sign_entropy)\n",
        "\n",
        "    sign_entropies = np.array(sign_entropies)\n",
        "\n",
        "    return sign_entropies\n",
        "\n",
        "\n",
        "def get_unstable_features(X, y, model, bs_indices, num_bootstraps=None):\n",
        "    coeffs_bs = []\n",
        "    if num_bootstraps != None:\n",
        "      print(\"bootstraps generated\")\n",
        "      bs_indices = []\n",
        "      for i in np.arange(0, num_bootstraps, step=1):\n",
        "        max_bs_range = X.shape[0]\n",
        "        indx_bs = random.choices(range(max_bs_range), k=max_bs_range)\n",
        "        bs_indices.append(indx_bs)\n",
        "\n",
        "    for i in range(len(bs_indices)):\n",
        "      indices = bs_indices[i]\n",
        "      X_sample, y_sample = X[indices], y[indices]\n",
        "      model.fit(X_sample, y_sample)\n",
        "      coeffs_bs.append(model.coef_)\n",
        "    coeffs_bs = np.array(coeffs_bs)\n",
        "\n",
        "    sign_entropies = []\n",
        "    for column in range(coeffs_bs.shape[1]):\n",
        "        data = coeffs_bs[:, column]\n",
        "        sign_entropy = calculate_entropy(data)\n",
        "        sign_entropies.append(sign_entropy)\n",
        "\n",
        "    num_predictors = len(sign_entropies)\n",
        "    sign_entropies = np.array(sign_entropies)\n",
        "    av_sign_entropy = np.mean(sign_entropies)\n",
        "    ratio_zero_entropy = np.count_nonzero(sign_entropies == 0) / (sign_entropies.shape[0])\n",
        "\n",
        "    non_zero_indices = np.where(sign_entropies != 0)[0]\n",
        "    zero_ent_indices = np.where(sign_entropies == 0)[0]\n",
        "\n",
        "\n",
        "\n",
        "    return non_zero_indices, zero_ent_indices, sign_entropies, coeffs_bs\n",
        "\n",
        "\n",
        "def evaluate_feature_selection_method(feature_selector, X_train, y_train, X_test, y_test,eval_indices_bs,\n",
        "                                      selector_params={}, model_params={}):\n",
        "    \"\"\"\n",
        "    Evaluate a given feature selection method on training and test data.\n",
        "\n",
        "    Parameters:\n",
        "    - feature_selector: function, a feature selection method that returns selected feature indices.\n",
        "    - X_train, y_train, X_test, y_test: train and test datasets.\n",
        "    - selector_params: dict, parameters for the feature selector.\n",
        "    - model_params: dict, parameters for the model.\n",
        "    - i: iteration index, useful for storing results in dictionaries.\n",
        "\n",
        "    Returns:\n",
        "    - rmse: Root Mean Squared Error using the selected features.\n",
        "    - sign_entropies: Sign entropies of the selected features.\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply feature selection\n",
        "    selected_indices = feature_selector(X_train, y_train, **selector_params)\n",
        "    # print(\"Selected Features: \", selected_indices)\n",
        "\n",
        "    # Subset data using selected features\n",
        "    X_train_selected = X_train[:, selected_indices]\n",
        "    X_test_selected = X_test[:, selected_indices]\n",
        "\n",
        "    # Fit and predict using the model\n",
        "    model = selector_params['estimator']\n",
        "    model.fit(X_train_selected, y_train)\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    evaluation_model = selector_params['evaluator']\n",
        "\n",
        "    _, _, sign_entropies, coeffs_bs = get_unstable_features(X_test_selected, y_test, evaluation_model, bs_indices=eval_indices_bs)\n",
        "\n",
        "    return selected_indices, rmse, sign_entropies, coeffs_bs\n",
        "\n",
        "\n",
        "def sefe_select_features(X_train, y_train, num_bootstrap=1000, slack=0.0, **kwargs):\n",
        "    model = kwargs['estimator']\n",
        "    num_iter = 10\n",
        "    tolerance_limit = 1\n",
        "    tolerance_cur=0\n",
        "    final_coeffs = np.zeros((num_bootstrap, X_train.shape[1]))\n",
        "\n",
        "    train_mat_sel_idx = np.zeros(X_train.shape[1])\n",
        "    for iter in range(num_iter):\n",
        "        zero_indices = np.where(train_mat_sel_idx == 0)[0] # get only those indices which had zero entropy from previous run\n",
        "\n",
        "        if len(zero_indices)==0:\n",
        "            print(\"No feature of zero entropy\")\n",
        "            break\n",
        "\n",
        "        coeffs_bs = []\n",
        "        for i in range(num_bootstrap):\n",
        "            indices_bs = random.choices(range(X_train.shape[0]), k=X_train.shape[0])\n",
        "            X_sample, y_sample = X_train[indices_bs][:, zero_indices], y_train[indices_bs]\n",
        "            model.fit(X_sample, y_sample)\n",
        "            coeffs_bs.append(model.coef_)\n",
        "\n",
        "        coeffs_bs = np.array(coeffs_bs)\n",
        "\n",
        "        sign_entropies = []\n",
        "        for column in range(coeffs_bs.shape[1]):\n",
        "            data = coeffs_bs[:, column]\n",
        "            sign_entropy = calculate_entropy(data)\n",
        "            sign_entropies.append(sign_entropy)\n",
        "\n",
        "        sign_entropies = np.array(sign_entropies)\n",
        "\n",
        "        non_zero_indices = np.where(sign_entropies != 0)[0]\n",
        "        zero_ent_indices = np.where(sign_entropies == 0)[0]\n",
        "\n",
        "        #non_zero_indices = np.where(sign_entropies > (0+slack))[0]\n",
        "        #zero_ent_indices = np.where(sign_entropies <= (0+slack))[0]\n",
        "\n",
        "        original_0_indices = np.where(train_mat_sel_idx == 0)[0]\n",
        "        mapped_non0_indices = original_0_indices[non_zero_indices]\n",
        "\n",
        "        if not np.size(mapped_non0_indices) == 0:\n",
        "            train_mat_sel_idx[mapped_non0_indices] = 1\n",
        "            tolerance_cur = 0\n",
        "        else:\n",
        "            if tolerance_cur == 0:\n",
        "                final_coeffs = coeffs_bs\n",
        "                tolerance_cur = tolerance_cur + 1\n",
        "            else:\n",
        "                if tolerance_cur < tolerance_limit:\n",
        "                    tolerance_cur = tolerance_cur + 1  # re-evaluating n times after a good run\n",
        "                else:\n",
        "                    break  # terminate the feature elimination process\n",
        "\n",
        "    top_idx = np.array(np.where(train_mat_sel_idx==0)[0])\n",
        "    if (len(top_idx)) == 0:\n",
        "        top_idx = np.arange(0, len(train_mat_sel_idx), step=1)\n",
        "\n",
        "    return top_idx\n",
        "\n",
        "\n",
        "def boruta_select_features_old(X_train, y_train, **kwargs):\n",
        "    k = kwargs.get('n_features_to_select', X_train.shape[1])  # default to all features if not provided\n",
        "    model = kwargs['estimator']\n",
        "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=0, random_state=42)\n",
        "    feat_selector.fit(X_train, y_train)\n",
        "    # Get ranking of the features\n",
        "    ranking = feat_selector.ranking_\n",
        "    # Sort features by ranking\n",
        "    top_k_idx = np.argsort(ranking)[:k]\n",
        "    return top_k_idx\n",
        "\n",
        "\n",
        "\n",
        "def rfe_select_features(X_train, y_train, **kwargs):\n",
        "    estimator = kwargs['estimator']\n",
        "    max_k = X_train.shape[1]  # Maximum number of features\n",
        "\n",
        "    param_grid = {'n_features_to_select': range(1, max_k + 1)}  # Define range of k values\n",
        "    selector = RFE(estimator=estimator)\n",
        "    grid_search = GridSearchCV(selector, param_grid, cv=5)  # 5-fold cross-validation\n",
        "    grid_search.fit(X_train, y_train)  # X_train and y_train are your training data\n",
        "\n",
        "    best_k = grid_search.best_params_['n_features_to_select']\n",
        "    selected_features = [i for i, selected in enumerate(grid_search.best_estimator_.support_) if selected]\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "\n",
        "def forward_select_features(X_train, y_train, **kwargs):\n",
        "    estimator = kwargs['estimator']\n",
        "    n_features_to_select = kwargs.get('n_features_to_select', X_train.shape[1])\n",
        "\n",
        "    selector = SequentialFeatureSelector(estimator=estimator, k_features=(1, X_train.shape[1]), cv=5, forward=True)\n",
        "    selector.fit(X_train, y_train)\n",
        "    return selector.k_feature_idx_\n",
        "\n",
        "\n",
        "def sbs_select_features(X_train, y_train, **kwargs):\n",
        "    estimator = kwargs['estimator']\n",
        "    n_features_to_select = kwargs.get('n_features_to_select', X_train.shape[1])\n",
        "    selector = SequentialFeatureSelector(estimator=estimator, k_features=(1, X_train.shape[1]), cv=5, forward=False)\n",
        "    selector.fit(X_train, y_train)\n",
        "    return selector.k_feature_idx_\n",
        "\n",
        "\n",
        "def bidirectional_select_features(X_train, y_train, **kwargs):\n",
        "    estimator = kwargs['estimator']\n",
        "    n_features_to_select = kwargs.get('n_features_to_select', X_train.shape[1])\n",
        "    selector = SequentialFeatureSelector(estimator=estimator, k_features=(1, X_train.shape[1]), cv=5, forward=True,\n",
        "                                         floating=True)\n",
        "    selector.fit(X_train, y_train)\n",
        "    return selector.k_feature_idx_\n",
        "\n",
        "\n",
        "def split_data_into_parts(X, y, K):\n",
        "    \"\"\"\n",
        "    Split the feature matrix X and target values y into K approximately equal-sized parts.\n",
        "\n",
        "    Parameters:\n",
        "        X (array-like): The feature matrix.\n",
        "        y (array-like): The target values.\n",
        "        K (int): The number of parts to split the data into.\n",
        "\n",
        "    Returns:\n",
        "        List of tuples: Each tuple contains the feature matrix and target values for one part.\n",
        "    \"\"\"\n",
        "    num_samples = X.shape[0]\n",
        "    part_size = num_samples // K\n",
        "    parts = []\n",
        "\n",
        "    # Shuffle the data randomly\n",
        "    indices = np.random.permutation(num_samples)\n",
        "    X_shuffled = X[indices]\n",
        "    y_shuffled = y[indices]\n",
        "\n",
        "    # Split the shuffled data into K parts\n",
        "    for i in range(K):\n",
        "        start_idx = i * part_size\n",
        "        end_idx = start_idx + part_size if i < K - 1 else num_samples\n",
        "        X_part = X_shuffled[start_idx:end_idx]\n",
        "        y_part = y_shuffled[start_idx:end_idx]\n",
        "        parts.append((X_part, y_part))\n",
        "\n",
        "    return parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOfOgxgtT08T"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'energy_data'\n",
        "# Load the dataset into a pandas DataFrame\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "\n",
        "# Extract features (X) and target variable (y)\n",
        "X = df.drop(columns=['Appliances', 'date', 'lights'])  # Remove 'Appliances', 'date', and 'lights' columns as features\n",
        "y = df['Appliances'].values  # Target variable is 'Appliances'\n",
        "\n",
        "threshold = 0.80\n",
        "X_reduced, removed_feature_names = remove_highly_correlated_features_pd(X,threshold)\n",
        "X = X_reduced\n",
        "print(removed_feature_names)\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL_TkIy7mbs5"
      },
      "outputs": [],
      "source": [
        "# Scale features\n",
        "X_min = X.min(axis=0)\n",
        "X_max = X.max(axis=0)\n",
        "X_scaled = (X - X_min) / (X_max - X_min)\n",
        "X = X_scaled\n",
        "\n",
        "csem_pert_range = 0.2\n",
        "num_perturb = 1000\n",
        "num_bootstraps = 1000\n",
        "csem_iter = 20\n",
        "num_runs = 1\n",
        "k=5 #not used in SEFE but used in others\n",
        "\n",
        "csem_params = {\n",
        "    'pert_range': csem_pert_range,\n",
        "    'n_perturb': num_perturb,\n",
        "    'n_iter': csem_iter\n",
        "}\n",
        "\n",
        "model_params = {\n",
        "    'alpha': 1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DOCXwgH2c22"
      },
      "outputs": [],
      "source": [
        "#K fold Cross Validation\n",
        "num_folds=5\n",
        "alpha_val = 0.1\n",
        "res_dict = {}\n",
        "selected_features_dict = {}\n",
        "rmse_dict = {}\n",
        "entropy_dict = {}\n",
        "coeffs_dict = {}\n",
        "for i in np.arange(0, num_runs, step=1):\n",
        "    print(\"################################################################   \" + str(i))\n",
        "    data_parts = split_data_into_parts(X, y, num_folds)\n",
        "    part_permutations = itertools.permutations(data_parts, 2)\n",
        "    for perm in part_permutations:\n",
        "        train_data = perm[0]\n",
        "        test_data = perm[1]\n",
        "\n",
        "        # Use train_data and test_data for training and testing your models\n",
        "        X_train, y_train = train_data\n",
        "        X_test, y_test = test_data\n",
        "\n",
        "        indices_bs = []\n",
        "        for j in range(num_bootstraps):\n",
        "            max_bs_range = X_test.shape[0]\n",
        "            indx_bs = random.choices(range(max_bs_range), k=max_bs_range)\n",
        "            indices_bs.append(indx_bs)\n",
        "\n",
        "        # SEFE\n",
        "        linear_reg_for_sefe = Ridge(alpha=alpha_val)#\n",
        "        selected_indices_sefe, rmse_sefe, sign_entropies_sefe, coeffs_bs_sefe = \\\n",
        "            evaluate_feature_selection_method(sefe_select_features, X_train, y_train,\n",
        "                                              X_test, y_test,\n",
        "                                              selector_params={'estimator': linear_reg_for_sefe,\n",
        "                                                               'evaluator': linear_reg_for_sefe,\n",
        "                                                               'n_features_to_select': k},\n",
        "                                              model_params=model_params,\n",
        "                                              eval_indices_bs=indices_bs)\n",
        "        k = len(selected_indices_sefe)\n",
        "\n",
        "        selected_features_dict.setdefault('sefe', []).append(selected_indices_sefe)\n",
        "        rmse_dict.setdefault('sefe', []).append(rmse_sefe)\n",
        "        entropy_dict.setdefault('sefe', []).append(sign_entropies_sefe)\n",
        "        coeffs_dict.setdefault('sefe', []).append(coeffs_bs_sefe)\n",
        "        #print(\"SEFE num features:\", selected_indices_sefe)\n",
        "        print(\"Entropy sefe:\", np.mean(sign_entropies_sefe))\n",
        "        print(\"rmse sefe:\", rmse_sefe)\n",
        "\n",
        "        # RFE\n",
        "        selected_indices_rfe, rmse_rfe, sign_entropies_rfe, coeffs_bs_rfe = evaluate_feature_selection_method(\n",
        "            rfe_select_features, X_train,\n",
        "            y_train, X_test, y_test, selector_params={'estimator': linear_reg_for_sefe,\n",
        "                                                      'evaluator': linear_reg_for_sefe,\n",
        "                                                      'n_features_to_select': k,\n",
        "                                                      'n_iter': csem_params['n_iter']},\n",
        "            model_params=model_params,\n",
        "            eval_indices_bs=indices_bs)\n",
        "        # rmse_dict[f'rfe_{i}'] = rmse_rfe\n",
        "        # entropy_dict[f'rfe_{i}'] = sign_entropies_rfe\n",
        "        # selected_features_dict[f'rfe_{i}'] = selected_indices_rfe\n",
        "        selected_features_dict.setdefault('rfe', []).append(selected_indices_rfe)\n",
        "        rmse_dict.setdefault('rfe', []).append(rmse_rfe)\n",
        "        entropy_dict.setdefault('rfe', []).append(sign_entropies_rfe)\n",
        "        coeffs_dict.setdefault('rfe', []).append(coeffs_bs_rfe)\n",
        "        #print(\"RFE num features:\", selected_indices_rfe)\n",
        "        print(\"Entropy rfe:\", np.mean(sign_entropies_rfe))\n",
        "        print(\"rmse rfe:\", rmse_rfe)\n",
        "\n",
        "\n",
        "        ##SFS\n",
        "        selected_indices_sfs, rmse_sfs, sign_entropies_sfs, coeffs_bs_sfs = evaluate_feature_selection_method(\n",
        "            forward_select_features, X_train, y_train, X_test, y_test,\n",
        "            selector_params={'estimator': linear_reg_for_sefe, 'evaluator': linear_reg_for_sefe,\n",
        "                             'n_features_to_select': k},\n",
        "            model_params=model_params, eval_indices_bs=indices_bs)\n",
        "        # selected_features_dict[f'sfs_{i}'] = selected_indices_sfs\n",
        "        # rmse_dict[f'sfs_{i}'] = rmse_sfs\n",
        "        # entropy_dict[f'sfs_{i}'] = sign_entropies_sfs\n",
        "        selected_features_dict.setdefault('sfs', []).append(selected_indices_sfs)\n",
        "        rmse_dict.setdefault('sfs', []).append(rmse_sfs)\n",
        "        entropy_dict.setdefault('sfs', []).append(sign_entropies_sfs)\n",
        "        coeffs_dict.setdefault('sfs', []).append(coeffs_bs_sfs)\n",
        "        print(\"Entropy SFS:\", np.mean(sign_entropies_sfs))\n",
        "        print(\"rmse SFS:\", rmse_sfs)\n",
        "\n",
        "        # SBS\n",
        "        selected_indices_sbs, rmse_sbs, sign_entropies_sbs, coeffs_bs_sbs = evaluate_feature_selection_method(\n",
        "            sbs_select_features, X_train, y_train, X_test, y_test,\n",
        "            selector_params={'estimator': linear_reg_for_sefe, 'evaluator': linear_reg_for_sefe,\n",
        "                             'n_features_to_select': k},\n",
        "            model_params=model_params, eval_indices_bs=indices_bs)\n",
        "        # selected_features_dict[f'sbs_{i}'] = selected_indices_sbs\n",
        "        # rmse_dict[f'sbs_{i}'] = rmse_sbs\n",
        "        # entropy_dict[f'sbs_{i}'] = sign_entropies_sbs\n",
        "        selected_features_dict.setdefault('sbs', []).append(selected_indices_sbs)\n",
        "        rmse_dict.setdefault('sbs', []).append(rmse_sbs)\n",
        "        entropy_dict.setdefault('sbs', []).append(sign_entropies_sbs)\n",
        "        coeffs_dict.setdefault('sbs', []).append(coeffs_bs_sbs)\n",
        "        print(\"Entropy SBS:\", np.mean(sign_entropies_sbs))\n",
        "        print(\"rmse SBS:\", rmse_sbs)\n",
        "\n",
        "        # Bidirectional\n",
        "        selected_indices_bidirectional, rmse_bidirectional, sign_entropies_bidirectional, coeffs_bs_bidirectional = evaluate_feature_selection_method(\n",
        "            bidirectional_select_features, X_train, y_train, X_test, y_test,\n",
        "            selector_params={'estimator': linear_reg_for_sefe, 'evaluator': linear_reg_for_sefe,\n",
        "                             'n_features_to_select': k},\n",
        "            model_params=model_params, eval_indices_bs=indices_bs)\n",
        "        # selected_features_dict[f'bidirectional_{i}'] = selected_indices_bidirectional\n",
        "        # rmse_dict[f'bidirectional_{i}'] = rmse_bidirectional\n",
        "        # entropy_dict[f'bidirectional_{i}'] = sign_entropies_bidirectional\n",
        "        selected_features_dict.setdefault('bidirectional', []).append(selected_indices_bidirectional)\n",
        "        rmse_dict.setdefault('bidirectional', []).append(rmse_bidirectional)\n",
        "        entropy_dict.setdefault('bidirectional', []).append(sign_entropies_bidirectional)\n",
        "        coeffs_dict.setdefault('bidirectional', []).append(coeffs_bs_bidirectional)\n",
        "        print(\"Entropy Bidirectional:\", np.mean(sign_entropies_bidirectional))\n",
        "        print(\"rmse bidir:\", rmse_bidirectional)\n",
        "\n",
        "\n",
        "res_dict = { 'selected_features': selected_features_dict,\n",
        "                            'rmse': rmse_dict,\n",
        "                            'entropy': entropy_dict,\n",
        "                            'coeffs': coeffs_dict\n",
        "                            }\n",
        "\n",
        "pkl_filename = dataset_name + '_CoFE_R' + str(alpha_val*10) + '_numfolds_CVk_' + str(num_folds) + '.pkl'\n",
        "print(pkl_filename)\n",
        "with open(pkl_filename, 'wb') as f1:\n",
        "  pickle.dump(res_dict, f1)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}